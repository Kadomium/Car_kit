{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "https://github.com/Kadomium/Car_kit/blob/main/Car_kit_prediction.ipynb",
      "authorship_tag": "ABX9TyNt+drVQNCU4C65iAssRDFy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kadomium/Car_kit/blob/main/Car_kit_prediction_null_drop.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ioe8345gDALv"
      },
      "outputs": [],
      "source": [
        "import datetime\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tensorflow.keras import Model, Sequential\n",
        "\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.losses import MeanSquaredError\n",
        "from tensorflow.keras.metrics import MeanAbsoluteError\n",
        "\n",
        "from tensorflow.keras.layers import Dense, Conv1D, LSTM, Lambda, Reshape, RNN, LSTMCell, Dropout\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install japanize-matplotlib\n",
        "import japanize_matplotlib"
      ],
      "metadata": {
        "id": "i6iNFZpdDIF1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b830e4c-1d0e-4586-b7b5-76903261e493"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting japanize-matplotlib\n",
            "  Downloading japanize-matplotlib-1.1.3.tar.gz (4.1 MB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/4.1 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.4/4.1 MB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m3.1/4.1 MB\u001b[0m \u001b[31m45.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.1/4.1 MB\u001b[0m \u001b[31m41.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from japanize-matplotlib) (3.10.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->japanize-matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->japanize-matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->japanize-matplotlib) (4.60.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->japanize-matplotlib) (1.4.9)\n",
            "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.12/dist-packages (from matplotlib->japanize-matplotlib) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->japanize-matplotlib) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->japanize-matplotlib) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->japanize-matplotlib) (3.2.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib->japanize-matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib->japanize-matplotlib) (1.17.0)\n",
            "Building wheels for collected packages: japanize-matplotlib\n",
            "  Building wheel for japanize-matplotlib (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for japanize-matplotlib: filename=japanize_matplotlib-1.1.3-py3-none-any.whl size=4120257 sha256=05aeff5bccba230cbedf754f6362db93143b12bbc9871661c7211d043824c24b\n",
            "  Stored in directory: /root/.cache/pip/wheels/c1/f7/9b/418f19a7b9340fc16e071e89efc379aca68d40238b258df53d\n",
            "Successfully built japanize-matplotlib\n",
            "Installing collected packages: japanize-matplotlib\n",
            "Successfully installed japanize-matplotlib-1.1.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.rcParams['figure.figsize'] = (10, 7.5)\n",
        "plt.rcParams['axes.grid'] = False"
      ],
      "metadata": {
        "id": "EGL8-HkzDLOV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Step 1: ファイルパスと設定の定義 ---\n",
        "new_file_path = 'carkit_null_drop.csv'\n",
        "chunk_size = 50000 # PCのスペックに応じて調整\n",
        "\n",
        "# 列名をリストとして取得\n",
        "new_all_columns = pd.read_csv(new_file_path, nrows=0).columns.tolist()\n",
        "\n",
        "# ---「削除する列」のリストを完成させる ---\n",
        "first_columns_to_drop = (['key','qqkey','qxq999','qs3','qsk4_1c','qsk4_2c','qs4_3','qs4_8a','qs4_9','qs4_10','qs4_11','qs4_12','qs4_13','qsq4_gas','qs4_4a','qskei_1','qskei_2','qsnou_1','qsnou_2','q6_2','q6_4']+[f'q6_5_3{x}{i}' for x in range(3, 6) for i in range(1, 22)]+['q9_2','q12_2']++[f'q13_1{i}' for i in range(1, 29)]+['q13_2','q14_2','qsq22_1'+'q29_2','q34_2','q42_4','qw1','qw2','qw11','qseinen','_src'])\n",
        "\n",
        "#q22に連続性の問題がなさそうなことから使用したい、バイナリデータも作成済み\n",
        "extra_columns_to_drop = ['qsk4_15','qs4_4a']\n",
        "\n",
        "# --- Step 5: 【作戦実行】「保持する列」リストを作成し、本番の処理へ ---\n",
        "# バイナル作成前に「保持すべき列」のリストを作成\n",
        "first_columns_to_keep = [col for col in new_all_columns if col not in first_columns_to_drop]\n",
        "columns_to_keep = [col for col in first_columns_to_keep if col not in extra_columns_to_drop]\n",
        "print(f\"\\n全1646列のうち、バイナル作成前に保持する列は {len(first_columns_to_keep)} 個です。\")\n",
        "print(f\"\\n全1646列のうち、バイナル作成後に保持する列は {len(columns_to_keep)} 個です。\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jaDW5th4OzyV",
        "outputId": "d948770b-f512-4038-967d-d8aa886b24df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "発見！前回データには存在しなかった新しい列が 25 個あります:\n",
            "Nullが1つ以上存在する列が 879 個見つかりました。\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "null1個以上:1126"
      ],
      "metadata": {
        "id": "Ohqmp4ZLebks"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#if 'qsk4_15'=428のバイナリを作成し、yとする（その後qsk4_15をdrop）\n",
        "#連続性をq22を中心に認めるなら、正直'_src'のdropの前にやることってなくない？"
      ],
      "metadata": {
        "id": "UK3CI9Pvlc9j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#ここも、もし乗っかればデータ型の最適化以外使わなくない？\n",
        "\n",
        "# --- Step 3: チャンクごとにデータを処理するループ ---\n",
        "# 処理済みのチャンクを格納するための空のリスト\n",
        "processed_chunks = []\n",
        "\n",
        "# イテレータ（繰り返し処理の仕組み）を作成\n",
        "# usecolsで、必要な列だけを読み込むように指定！\n",
        "chunk_iterator = pd.read_csv(new_file_path, chunksize=chunk_size, usecols=first_columns_to_keep)\n",
        "\n",
        "#print(\"--- 巨大ファイルのチャンク処理を開始 ---\")\n",
        "for i, chunk in enumerate(chunk_iterator):\n",
        "    #print(f\"チャンク {i+1} を処理中 (行数: {len(chunk)})...\")\n",
        "\n",
        "    # --- ここで、各チャンクに対して必要な前処理を行う ---\n",
        "\n",
        "    # (例) データ型の最適化 (メモリ削減に超重要！)\n",
        "    for col in chunk.select_dtypes(include=['int64']).columns:\n",
        "        chunk[col] = pd.to_numeric(chunk[col], downcast='integer')\n",
        "    for col in chunk.select_dtypes(include=['float64']).columns:\n",
        "        chunk[col] = pd.to_numeric(chunk[col], downcast='float')\n",
        "    for col in chunk.select_dtypes(include=['object']).columns:\n",
        "        if chunk[col].nunique() / len(chunk) < 0.5:\n",
        "            chunk[col] = chunk[col].astype('category')\n",
        "\n",
        "    # (例) Nullの処理（もし必要なら）\n",
        "    # chunk.dropna(subset=['important_column'], inplace=True)\n",
        "\n",
        "    # 処理済みのチャンクをリストに追加\n",
        "    processed_chunks.append(chunk)\n",
        "\n",
        "#print(\"\\n--- 全てのチャンク処理が完了 ---\")\n",
        "\n",
        "\n",
        "# --- Step 4: 処理済みの全チャンクを一つのデータフレームに結合 ---\n",
        "#print(\"処理済みチャンクを結合中...\")\n",
        "df_cleaned = pd.concat(processed_chunks, ignore_index=True)\n",
        "##print(\"結合完了！\")\n",
        "\n",
        "#print(\"\\n--- 最終的なデータフレーム情報 ---\")\n",
        "df_cleaned.info(memory_usage='deep')"
      ],
      "metadata": {
        "id": "MMZvEFrNObMf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff16ee02-5e12-472d-b84c-28ee290fce2b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 77512 entries, 0 to 77511\n",
            "Columns: 1321 entries, qstrend to _src\n",
            "dtypes: category(1), float32(667), int16(1), int8(652)\n",
            "memory usage: 245.6 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_cleaned.loc[298143, '_src'] = '20250831_Car-kit_2023-2025_raw.dta'"
      ],
      "metadata": {
        "id": "ZLm3zOtY9s-j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_cleaned.tail()"
      ],
      "metadata": {
        "id": "0iSlmDiC-Cz0",
        "outputId": "6fbecb0e-0ce7-4eca-b67f-ab7963550fc1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 429
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        qstrend  qxxx_1  qsk4_15  qs4_4a  qs7  qs7_2  qs8  qs8_2  qs9  qxxx_2  \\\n",
              "298139      126     NaN      951     130    1      2    1      1    1     NaN   \n",
              "298140      126     NaN      335     130    1      1    2      1    1     NaN   \n",
              "298141      126     NaN      436     130    2      3    1      1    1     NaN   \n",
              "298142      126     NaN      995     130    1      1    1      1    2     NaN   \n",
              "298143      126     NaN      257     130    1      1    1      1    1     NaN   \n",
              "\n",
              "        ...  q57_114  q57_115  q57_116  q57_117  q57_118   qw3  qw5p  qw6  \\\n",
              "298139  ...      0.0      0.0      0.0      0.0      0.0  46.0  13.0  1.0   \n",
              "298140  ...      0.0      0.0      0.0      0.0      0.0   8.0   4.0  1.0   \n",
              "298141  ...      0.0      0.0      0.0      0.0      0.0   3.0   7.0  1.0   \n",
              "298142  ...      0.0      0.0      0.0      0.0      0.0  21.0   1.0  1.0   \n",
              "298143  ...      NaN      NaN      NaN      NaN      NaN   NaN   NaN  NaN   \n",
              "\n",
              "         qw7                                _src  \n",
              "298139  10.0  20250831_Car-kit_2023-2025_raw.dta  \n",
              "298140   3.0  20250831_Car-kit_2023-2025_raw.dta  \n",
              "298141   2.0  20250831_Car-kit_2023-2025_raw.dta  \n",
              "298142   6.0  20250831_Car-kit_2023-2025_raw.dta  \n",
              "298143   NaN  20250831_Car-kit_2023-2025_raw.dta  \n",
              "\n",
              "[5 rows x 1321 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e9c93fbf-bec0-4bee-b0b0-62549a983a24\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>qstrend</th>\n",
              "      <th>qxxx_1</th>\n",
              "      <th>qsk4_15</th>\n",
              "      <th>qs4_4a</th>\n",
              "      <th>qs7</th>\n",
              "      <th>qs7_2</th>\n",
              "      <th>qs8</th>\n",
              "      <th>qs8_2</th>\n",
              "      <th>qs9</th>\n",
              "      <th>qxxx_2</th>\n",
              "      <th>...</th>\n",
              "      <th>q57_114</th>\n",
              "      <th>q57_115</th>\n",
              "      <th>q57_116</th>\n",
              "      <th>q57_117</th>\n",
              "      <th>q57_118</th>\n",
              "      <th>qw3</th>\n",
              "      <th>qw5p</th>\n",
              "      <th>qw6</th>\n",
              "      <th>qw7</th>\n",
              "      <th>_src</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>298139</th>\n",
              "      <td>126</td>\n",
              "      <td>NaN</td>\n",
              "      <td>951</td>\n",
              "      <td>130</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>46.0</td>\n",
              "      <td>13.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>20250831_Car-kit_2023-2025_raw.dta</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>298140</th>\n",
              "      <td>126</td>\n",
              "      <td>NaN</td>\n",
              "      <td>335</td>\n",
              "      <td>130</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>20250831_Car-kit_2023-2025_raw.dta</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>298141</th>\n",
              "      <td>126</td>\n",
              "      <td>NaN</td>\n",
              "      <td>436</td>\n",
              "      <td>130</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>20250831_Car-kit_2023-2025_raw.dta</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>298142</th>\n",
              "      <td>126</td>\n",
              "      <td>NaN</td>\n",
              "      <td>995</td>\n",
              "      <td>130</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>21.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>20250831_Car-kit_2023-2025_raw.dta</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>298143</th>\n",
              "      <td>126</td>\n",
              "      <td>NaN</td>\n",
              "      <td>257</td>\n",
              "      <td>130</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>20250831_Car-kit_2023-2025_raw.dta</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 1321 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e9c93fbf-bec0-4bee-b0b0-62549a983a24')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-e9c93fbf-bec0-4bee-b0b0-62549a983a24 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-e9c93fbf-bec0-4bee-b0b0-62549a983a24');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-ef8497be-e36c-409c-abe6-82f37c11b3fe\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-ef8497be-e36c-409c-abe6-82f37c11b3fe')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-ef8497be-e36c-409c-abe6-82f37c11b3fe button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#qsk4_15=428がN-BOX、430がN-BOX Custom\n",
        "# qsk4_15が428なら1、そうでなければ0の列を作成\n",
        "\n",
        "df_cleaned['target_count_custom'] = (df_cleaned['qsk4_15'] == 430).astype(int)\n",
        "\n",
        "# ★★★★★ このタイミングで 'qsk4_15' を完全に削除する ★★★★★\n",
        "df_cleaned.drop(columns=['qsk4_15'], inplace=True)\n",
        "#\n",
        "columns_to_keep = first_columns_to_keep"
      ],
      "metadata": {
        "id": "N-vDxBOY-GRz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. 各列のnullの数を計算\n",
        "nan_counts = df_cleaned.isnull().sum()\n",
        "# 2. nullの数を降順でソートして表示\n",
        "print(\"--- 各列のNullの数（降順） ---\")\n",
        "print(nan_counts.sort_values(ascending=False))\n",
        "\n",
        "# 3. データ総数に対するnullの割合を計算して表示\n",
        "total_rows = len(df_cleaned)\n",
        "nan_ratio = (df_cleaned.isnull().sum() / total_rows) * 100\n",
        "print(\"\\n--- 各列のNullの割合（%） ---\")\n",
        "print(nan_ratio.sort_values(ascending=False))"
      ],
      "metadata": {
        "id": "dU-gqOaHoFWj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 残った列で再度、欠損率を計算\n",
        "nan_ratio = df.isnull().sum() / len(df)\n",
        "\n",
        "# 可視化1: ヒストグラムで欠損率の分布を見る\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.histplot(nan_ratio * 100, bins=50, kde=True)\n",
        "plt.title('欠損率の分布（100%欠損の列は削除済み）', fontsize=16)\n",
        "plt.xlabel('欠損率 (%)', fontsize=12)\n",
        "plt.ylabel('質問項目の数', fontsize=12)\n",
        "plt.grid(True)\n",
        "plt.savefig(\"null_hist.png\", dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# 可視化2: 欠損率を降順にソートした棒グラフで見る\n",
        "plt.figure(figsize=(12, 6))\n",
        "nan_ratio.sort_values(ascending=False).plot(kind='line', use_index=False)\n",
        "plt.title('欠損率の降順プロット（エルボーポイントを探す）', fontsize=16)\n",
        "plt.xlabel('質問項目（欠損率の高い順）', fontsize=12)\n",
        "plt.ylabel('欠損率', fontsize=12)\n",
        "plt.grid(True)\n",
        "plt.savefig(\"null_elbow.png\", dpi=300, bbox_inches='tight')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7IRbva8uwF0e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Step 1: 時間コードと実際の年月を対応付ける辞書を作成 ---\n",
        "# このマッピングを37ヶ月分作成します\n",
        "start_year = 2017\n",
        "start_month = 12\n",
        "time_map = {}\n",
        "for i in range(91):\n",
        "    current_year = start_year + (start_month + i - 1) // 12\n",
        "    current_month = (start_month + i - 1) % 12 + 1\n",
        "\n",
        "    time_code = 54 + i\n",
        "    date_str = f\"{current_year}-{current_month:02d}\" # '2022-01' のように0埋めする\n",
        "\n",
        "    time_map[time_code] = date_str\n",
        "\n",
        "# --- Step 2: 辞書を使って 'qs4_4a'列を置換（マッピング） ---\n",
        "# .map()メソッドを使うのが最も簡単で効率的です\n",
        "df_cleaned['date_str'] = df_cleaned['qs4_4a'].map(time_map)\n",
        "\n",
        "# --- Step 3: 【最重要】文字列をPandasのdatetime型（月末実績値）に変換 ---\n",
        "df_cleaned['date'] = pd.to_datetime(df_cleaned['date_str']).dt.to_period('M').dt.to_timestamp('M')\n",
        "\n",
        "# --- Step 4: datetime型の列をデータフレームのインデックスに設定 ---\n",
        "# これにより、データが正式に時系列データとして扱われます\n",
        "df_cleaned.set_index('date', inplace=True)\n",
        "\n",
        "# 元の不要になった列は削除して整理\n",
        "df_cleaned.drop(['qs4_4a', 'date_str'], axis=1, inplace=True)\n",
        "\n",
        "print(\"\\n--- 最終的な時系列データフレーム ---\")\n",
        "print(df_cleaned.head())\n",
        "print(\"\\nインデックスの型:\")\n",
        "print(type(df_cleaned.index))"
      ],
      "metadata": {
        "id": "kOOuPjTA9UwO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1の分類に基づき、集計戦略を立てる\n",
        "# まず、4.多クラスのカテゴリデータ(職業、最も運転する人など)をOne-Hotエンコーディングする\n",
        "df_cleaned = pd.get_dummies(df_cleaned, columns=['qs9','q13_2a','qw3','qw7'])\n",
        "\n",
        "# Step 2: .agg() を使って一気に集計する\n",
        "#エンコーディング済みはqxx_iのように下に_（選択肢）がつく\n",
        "# 集計ルールを辞書で定義\n",
        "#1. 連続する数値データ(年齢)　は : 'mean', 外れ値が嫌なら : 'median',\n",
        "#2. 段階的な数値データ(満足度、こだわり度) : 'mean',\n",
        "#3. バイナリデータは : 'mean',\n",
        "\n",
        "aggregation_rules = {\n",
        "    col: 'sum' if col == 'target_count_custom' else 'mean'\n",
        "    for col in columns_to_aggregate\n",
        "}\n",
        "\n",
        "# 'M' (Month End) でリサンプリングし、定義したルールで集計\n",
        "df_monthly = df_cleaned.resample('M').agg(aggregation_rules)\n",
        "\n",
        "print(\"--- 月次に集計されたデータフレーム ---\")\n",
        "print(df_monthly.head())"
      ],
      "metadata": {
        "id": "A1WfnCrOIbYf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Step 1: まず「特徴量X」と「正解ラベルy」に分離する ---\n",
        "X = df_monthly.drop('target_count_custom', axis=1) # target_count_custom列以外、すべてが特徴量X\n",
        "y = df_monthly[['target_count_custom']] # target_count_custom列だけが正解ラベルy (DataFrame形式で抽出)\n",
        "\n",
        "print(\"--- 1. 特徴量Xと正解ラベルyに分離 ---\")\n",
        "print(\"X (問題用紙) の形状:\", X.shape)\n",
        "print(\"y (模範解答) の形状:\", y.shape)\n",
        "\n",
        "\n",
        "# --- Step 2: Xとyを、同じ分割点で7:2:1に分割する ---\n",
        "n_data = len(df_monthly)\n",
        "train_split = int(n_data * 0.7)\n",
        "val_split = int(n_data * 0.9)\n",
        "\n",
        "X_train, X_val, X_test = X[:train_split], X[train_split:val_split], X[val_split:]\n",
        "y_train, y_val, y_test = y[:train_split], y[train_split:val_split], y[val_split:]\n",
        "\n",
        "print(f\"\\n--- 2. Xとyを同じように時間で分割 ---\")\n",
        "print(f\"学習データ: X_train:{X_train.shape}, y_train:{y_train.shape}\")\n",
        "print(f\"検証データ: X_val:{X_val.shape}, y_val:{y_val.shape}\")\n",
        "print(f\"テストデータ: X_test:{X_test.shape}, y_test:{y_test.shape}\")\n",
        "\n",
        "\n",
        "# --- Step 3: Xとyを「別々の」Scalerで正規化する ---\n",
        "# ★★★★★ なぜ別々？ -> 後でyの予測結果を元のスケールに戻すため ★★★★★\n",
        "scaler_X = MinMaxScaler(feature_range=(0, 1))\n",
        "scaler_y = MinMaxScaler(feature_range=(0, 1))\n",
        "\n",
        "# (3-a) Scalerを「学習データだけ」にフィットさせる\n",
        "scaler_X.fit(X_train)\n",
        "scaler_y.fit(y_train)\n",
        "\n",
        "# (3-b) 学習したScalerを使って、全てのデータを変形(transform)する\n",
        "X_train_scaled = scaler_X.transform(X_train)\n",
        "X_val_scaled = scaler_X.transform(X_val)\n",
        "X_test_scaled = scaler_X.transform(X_test)\n",
        "\n",
        "y_train_scaled = scaler_y.transform(y_train)\n",
        "y_val_scaled = scaler_y.transform(y_val)\n",
        "y_test_scaled = scaler_y.transform(y_test)\n",
        "\n",
        "print(f\"\\n--- 3. 正しく正規化されたXとyのデータ ---\")\n",
        "print(\"X_train_scaledの形状:\", X_train_scaled.shape)\n",
        "print(\"y_train_scaledの形状:\", y_train_scaled.shape) # LSTMに入力するため、こちらも正規化"
      ],
      "metadata": {
        "id": "4SK6vJr3BxWo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Step 1: ルックバック期間の定義 ---\n",
        "lookback = 3 # 過去3ヶ月分のデータを使って予測する\n",
        "\n",
        "\n",
        "# --- Step 2: シーケンスデータを作成する関数を定義 ---\n",
        "def create_sequences(X_data, y_data, lookback):\n",
        "    X_seq, y_seq = [], []\n",
        "    # ループは lookback から開始し、データの最後まで\n",
        "    for i in range(lookback, len(X_data)):\n",
        "        # i-lookback から i までが入力シーケンス (問題)\n",
        "        X_seq.append(X_data[i-lookback:i, :])\n",
        "        # i 番目が正解ラベル (答え)\n",
        "        y_seq.append(y_data[i, 0])\n",
        "\n",
        "    return np.array(X_seq), np.array(y_seq)\n",
        "\n",
        "\n",
        "# --- Step 3: 関数を各データセットに適用 ---\n",
        "X_train_seq, y_train_seq = create_sequences(X_train_scaled, y_train_scaled, lookback)\n",
        "X_val_seq, y_val_seq = create_sequences(X_val_scaled, y_val_scaled, lookback)\n",
        "X_test_seq, y_test_seq = create_sequences(X_test_scaled, y_test_scaled, lookback)\n",
        "\n",
        "\n",
        "# --- Step 4: 結果の形状を確認 ---\n",
        "print(\"--- シーケンスデータへの変換後 ---\")\n",
        "print(\"【学習用データ】\")\n",
        "print(f\"X_train_seq shape: {X_train_seq.shape}\")\n",
        "print(f\"y_train_seq shape: {y_train_seq.shape}\")\n",
        "print(f\"  -> {X_train_seq.shape[0]}個の「問題と答えのペア」が作成されました。\")\n",
        "\n",
        "print(\"\\n【検証用データ】\")\n",
        "print(f\"X_val_seq shape: {X_val_seq.shape}\")\n",
        "print(f\"y_val_seq shape: {y_val_seq.shape}\")\n",
        "print(f\"  -> {X_val_seq.shape[0]}個の「問題と答えのペア」が作成されました。\")\n",
        "\n",
        "print(\"\\n【テスト用データ】\")\n",
        "print(f\"X_test_seq shape: {X_test_seq.shape}\")\n",
        "print(f\"y_test_seq shape: {y_test_seq.shape}\")\n",
        "print(f\"  -> {X_test_seq.shape[0]}個の「問題と答えのペア」が作成されました。\")"
      ],
      "metadata": {
        "id": "RILI9FtPESi3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- モデルの器を定義 ---\n",
        "model = Sequential()\n",
        "\n",
        "# --- 入力層 兼 1層目のLSTM ---\n",
        "# units: LSTM層のニューロン数。多すぎず少なすぎず64あたりから始めるのが一般的。\n",
        "# input_shape: モデルへの最初の入力の形状。(タイムステップ数, 特徴量数)\n",
        "model.add(LSTM(units=64, return_sequences=True, input_shape=(X_train_seq.shape[1], X_train_seq.shape[2])))\n",
        "model.add(Dropout(0.2)) # 20%のニューロンをランダムに無効化\n",
        "\n",
        "# --- 2層目のLSTM ---\n",
        "model.add(LSTM(units=64, return_sequences=False))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "# --- 全結合層 ---\n",
        "model.add(Dense(units=16, activation='relu')) # reluは標準的な活性化関数\n",
        "\n",
        "# --- 出力層 ---\n",
        "model.add(Dense(units=1)) # 最終的な予測値は1つなのでunits=1\n",
        "\n",
        "\n",
        "# --- モデルのコンパイル ---\n",
        "# optimizer: どうやって学習を進めるかのアルゴリズム。'adam'が最も一般的で高性能。\n",
        "# loss: モデルの予測がどれだけ間違っているかの指標。回帰問題では'mean_squared_error'が標準。\n",
        "model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "\n",
        "# --- モデルの設計図を確認 ---\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "b_qdHO5WI5oz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- EarlyStoppingの設定 ---\n",
        "# patience=5: 5エポック連続で検証ロスの改善が見られなければ学習を停止\n",
        "# restore_best_weights=True: 停止した際、最も検証ロスが良かった時点のモデルの重みに戻す\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "\n",
        "\n",
        "# --- モデルの学習を開始 ---\n",
        "history = model.fit(\n",
        "    X_train_seq, y_train_seq,\n",
        "    epochs=100, # 多めに設定してもEarlyStoppingが止めてくれる\n",
        "    batch_size=8, # データが少ないのでバッチサイズも小さく\n",
        "    validation_data=(X_val_seq, y_val_seq),\n",
        "    callbacks=[early_stopping],\n",
        "    verbose=1 # 学習の進捗を表示\n",
        ")\n",
        "\n",
        "print(\"\\n--- モデルの学習が完了しました！ ---\")"
      ],
      "metadata": {
        "id": "mNXORC3jKm2u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Step 1: テストデータで予測を実行 ---\n",
        "# model.predict() は、正規化されたスケール(0-1)で予測値を返します\n",
        "predicted_scaled = model.predict(X_test_seq)\n",
        "\n",
        "\n",
        "# --- Step 2: ★★★★★ 最重要 ★★★★★ 予測値を元のスケールに戻す ---\n",
        "# 人間が理解できる「販売台数」のスケールに変換します\n",
        "# このために、y専用のScaler (scaler_y) を使います\n",
        "predicted_actual = scaler_y.inverse_transform(predicted_scaled)\n",
        "\n",
        "\n",
        "# --- Step 3: 実際の正解データも元のスケールに戻して比較 ---\n",
        "# y_test_scaled を使っても良いですが、分割前の y_test を使うのが簡単です\n",
        "actual_data = y_test[-len(y_test_seq):] # シーケンス作成で削られた分を考慮\n",
        "# もしくは、y_test_scaledを逆変換\n",
        "# actual_data_from_scaled = scaler_y.inverse_transform(y_test_scaled.reshape(-1, 1))\n",
        "\n",
        "\n",
        "print(\"\\n--- 最終テスト結果 ---\")\n",
        "# X_test_seqは1つしかないので、予測結果も1つだけ出てきます\n",
        "print(f\"予測された来月の販売台数: {predicted_actual[0][0]:.2f} 台\")\n",
        "print(f\"実際の来月の販売台数: {actual_data.iloc[0]['target_count_custom']} 台\")\n",
        "\n",
        "# 誤差を計算\n",
        "error = predicted_actual[0][0] - actual_data.iloc[0]['target_count_custom']\n",
        "print(f\"予測誤差: {error:.2f} 台\")"
      ],
      "metadata": {
        "id": "DScD-Wi7MGLg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 1-a: 全ての正規化済みデータを結合する ---\n",
        "X_full_scaled = np.concatenate([X_train_scaled, X_val_scaled, X_test_scaled], axis=0)\n",
        "y_full_scaled = np.concatenate([y_train_scaled, y_val_scaled, y_test_scaled], axis=0)\n",
        "\n",
        "print(\"--- 全データの形状 ---\")\n",
        "print(f\"X_full_scaled shape: {X_full_scaled.shape}\") # -> (37, 962)\n",
        "print(f\"y_full_scaled shape: {y_full_scaled.shape}\") # -> (37, 1)\n",
        "\n",
        "# --- 1-b: 結合した全データで、シーケンスを再作成 ---\n",
        "lookback = 3 # 開発時と同じlookbackを使う\n",
        "X_full_seq, y_full_seq = create_sequences(X_full_scaled, y_full_scaled, lookback)\n",
        "\n",
        "print(\"\\n--- 全データで作ったシーケンスの形状 ---\")\n",
        "print(f\"X_full_seq shape: {X_full_seq.shape}\") # -> (34, 3, 962)\n",
        "print(f\"y_full_seq shape: {y_full_seq.shape}\") # -> (34,)\n",
        "\n",
        "\n",
        "# --- 1-c: モデルを「再定義」し、「再学習」させる ---\n",
        "# 以前と同じ設計図で、新しいモデルを組み立てる\n",
        "model_final = Sequential([\n",
        "    LSTM(units=64, return_sequences=True, input_shape=(X_full_seq.shape[1], X_full_seq.shape[2])),\n",
        "    Dropout(0.2),\n",
        "    LSTM(units=32, return_sequences=False),\n",
        "    Dropout(0.2),\n",
        "    Dense(units=16, activation='relu'),\n",
        "    Dense(units=1)\n",
        "])\n",
        "\n",
        "model_final.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "# 全データを使って学習 (今回は検証データはないので、validation_dataは不要)\n",
        "print(\"\\n--- 最終版モデルの学習開始！ ---\")\n",
        "model_final.fit(\n",
        "    X_full_seq, y_full_seq,\n",
        "    epochs=100, # EarlyStoppingがないので、ロスが下がらなくなるのを見計らって手動で止めても良い\n",
        "    batch_size=8,\n",
        "    verbose=1\n",
        ")\n",
        "print(\"\\n--- 最終版モデルの学習完了！ ---\")"
      ],
      "metadata": {
        "id": "5cdFy3l-g1D8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 2-a: 予測の「元」となる最後の3ヶ月分のデータを準備 ---\n",
        "# 元の月次集計データフレーム (正規化する前) から最後の3件を取得\n",
        "last_sequence_raw = df_monthly.iloc[-lookback:]\n",
        "\n",
        "# ★★★★★ 開発時に使った「scaler_X」で正規化する ★★★★★\n",
        "# 新しくscalerをfitさせてはいけない！過去の基準で正規化するのが重要\n",
        "last_sequence_scaled = scaler_X.transform(last_sequence_raw.drop('target_count_custom', axis=1))\n",
        "\n",
        "\n",
        "# --- 2-b: LSTMの入力形式 (3次元) に変形 ---\n",
        "# (1, タイムステップ数, 特徴量数) という形状にする\n",
        "# 「1」は、これから1つだけ予測を行う、という意味\n",
        "input_for_prediction = last_sequence_scaled.reshape(1, lookback, X_full_seq.shape[2])\n",
        "\n",
        "print(\"\\n--- 未来予測用の入力データ ---\")\n",
        "print(f\"形状: {input_for_prediction.shape}\")\n",
        "\n",
        "\n",
        "# --- 2-c: 最終予測の実行！ ---\n",
        "predicted_future_scaled = model_final.predict(input_for_prediction)\n",
        "\n",
        "# --- 2-d: 予測値を元の「販売台数」スケールに戻す ---\n",
        "# これも開発時に使った「scaler_y」を使う\n",
        "predicted_future_actual = scaler_y.inverse_transform(predicted_future_scaled)\n",
        "\n",
        "\n",
        "print(\"\\n\\n\" + \"=\"*40)\n",
        "print(\"     未来予測の結果\")\n",
        "print(\"=\"*40)\n",
        "print(f\"AIが予測する【2025年2月】の販売台数: {predicted_future_actual[0][0]:.2f} 台\")\n",
        "print(\"=\"*40)"
      ],
      "metadata": {
        "id": "YIVe07Zyg5oO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 最後の3ヶ月分の入力データ (前のステップで作成済み)\n",
        "input_for_prediction = last_sequence_scaled.reshape(1, lookback, X_full_seq.shape[2])\n",
        "\n",
        "# 何ヶ月先まで予測したいか\n",
        "n_future_steps = 3\n",
        "\n",
        "# 予測結果を格納するリスト\n",
        "future_predictions = []\n",
        "\n",
        "# 現在の入力を保持する変数\n",
        "current_input = input_for_prediction.copy()\n",
        "\n",
        "print(\"--- 再帰的予測を開始 ---\")\n",
        "for i in range(n_future_steps):\n",
        "    # 1. 次の1ヶ月を予測\n",
        "    next_step_pred_scaled = model_final.predict(current_input)\n",
        "\n",
        "    # 2. 予測結果をリストに保存\n",
        "    future_predictions.append(next_step_pred_scaled[0])\n",
        "\n",
        "    # 3. 次の予測のための新しい入力を作成\n",
        "    #    - 入力の最後の特徴量ベクトルに、今回の予測値を差し込む (ここは簡易的な方法)\n",
        "    #    - 注：実際には他の961個の特徴量は不明なので、前の月の値を使うなど仮定が必要。\n",
        "    #      ここではターゲット(y)だけを更新する簡易版を示します。\n",
        "    #      より厳密には、特徴量も予測するモデルが必要になります。\n",
        "\n",
        "    # 簡易的な次の入力の作成 (ターゲット予測値だけを更新する例)\n",
        "    # 実際にはもっと複雑な処理が必要になる場合が多い\n",
        "    new_features_for_next_step = current_input[0, -1, :].copy() # 最後の月の特徴量\n",
        "    # 本来はこの部分でy以外の特徴量も予測・更新する必要がある\n",
        "\n",
        "    # ここでは簡易化のため、入力シーケンスを1つずらし、\n",
        "    # 新しい予測値を（仮の特徴量ベクトルとして）追加するイメージで進めます。\n",
        "    # 注：この方法はあくまでコンセプトを示すもので、特徴量の扱いには注意が必要です。\n",
        "\n",
        "    # よりシンプルな方法は、入力シーケンスを「ずらす」ことです\n",
        "    # 最初のタイムステップを削除\n",
        "    next_input_features = current_input[0, 1:, :]\n",
        "\n",
        "    # 予測結果を新しいタイムステップとして追加\n",
        "    # 予測結果はyのみなので、xの形状に合わせる必要がある（ここではダミー値で代用）\n",
        "    predicted_y = next_step_pred_scaled[0][0]\n",
        "    dummy_x_features = np.zeros(current_input.shape[2] - 1) # y以外の特徴量を0で埋めるダミー\n",
        "    new_timestep = np.insert(dummy_x_features, 0, predicted_y).reshape(1, current_input.shape[2])\n",
        "\n",
        "    current_input = np.append(next_input_features, new_timestep, axis=0).reshape(1, lookback, X_full_seq.shape[2])\n",
        "\n",
        "\n",
        "# 予測結果を元のスケールに戻す\n",
        "future_predictions_actual = scaler_y.inverse_transform(np.array(future_predictions))\n",
        "\n",
        "print(\"\\n--- 複数ステップ先の予測結果 ---\")\n",
        "for i, pred in enumerate(future_predictions_actual):\n",
        "    print(f\"【{i+1}ヶ月先】の予測販売台数: {pred[0]:.2f} 台\")"
      ],
      "metadata": {
        "id": "yPaD0eJGi8R-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_future_steps = 12 # 3ヶ月先まで予測する\n",
        "future_predictions = []\n",
        "\n",
        "# 履歴を保持するためのデータフレーム（これに予測結果を追記していく）\n",
        "df_history = df_monthly.copy()\n",
        "\n",
        "# 予測ループの開始\n",
        "for i in range(n_future_steps):\n",
        "\n",
        "    # --- 1-a: 次の月の特徴量Xを作成する ---\n",
        "    last_month = df_history.iloc[-1]\n",
        "    # 12ヶ月前（前年同月）と13ヶ月前のデータを取得\n",
        "    last_year_same_month = df_history.iloc[-12]\n",
        "\n",
        "    # ★★★★★ あなたの戦略Aを実装 ★★★★★\n",
        "    future_X_values = (last_year_same_month + last_month) / 2\n",
        "\n",
        "    # ターゲット列はまだ不明なので、一旦仮の値（0など）を入れておく\n",
        "    future_X_df = pd.DataFrame(future_X_values).T\n",
        "    future_X_df['target_count_custom'] = 0 # ターゲット列はXに含まない\n",
        "    future_X_for_scale = future_X_df.drop('target_count_custom', axis=1)\n",
        "\n",
        "\n",
        "    # --- 1-b: 予測の入力シーケンスを作成 ---\n",
        "    # 履歴の最後から (lookback-1) 件を取得\n",
        "    last_sequence_raw = df_history.iloc[-(lookback-1):].drop('target_count_custom', axis=1)\n",
        "    # 1-aで作成した未来の特徴量と結合\n",
        "    input_sequence_raw = pd.concat([last_sequence_raw, future_X_for_scale])\n",
        "\n",
        "\n",
        "    # --- 1-c: 正規化と形状変換 ---\n",
        "    input_sequence_scaled = scaler_X.transform(input_sequence_raw)\n",
        "    input_for_prediction = input_sequence_scaled.reshape(1, lookback, df_monthly.shape[1] - 1)\n",
        "\n",
        "\n",
        "    # --- 1-d: 予測の実行と保存 ---\n",
        "    predicted_scaled = model_final.predict(input_for_prediction)\n",
        "    predicted_actual = scaler_y.inverse_transform(predicted_scaled)\n",
        "    future_predictions.append(predicted_actual[0][0])\n",
        "\n",
        "\n",
        "    # --- 1-e: 履歴の更新 ★★★★★\n",
        "    # 次のループのために、今予測した値を履歴に追加する\n",
        "    new_row = future_X_df.copy()\n",
        "    new_row.index = [df_history.index[-1] + pd.DateOffset(months=1)]\n",
        "    new_row['target_count_custom'] = predicted_actual[0][0]\n",
        "\n",
        "    df_history = pd.concat([df_history, new_row])\n",
        "\n",
        "\n",
        "print(\"\\n\" + \"=\"*40)\n",
        "print(f\"     未来{n_future_steps}ヶ月の再帰的予測結果\")\n",
        "print(\"=\"*40)\n",
        "for i, pred in enumerate(future_predictions):\n",
        "    future_month = df_monthly.index[-1] + pd.DateOffset(months=i+1)\n",
        "    print(f\"【{future_month.strftime('%Y-%m')}】の予測販売台数: {pred:.2f} 台\")\n",
        "print(\"=\"*40)"
      ],
      "metadata": {
        "id": "hAgW03D1qyjc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.font_manager as fm\n",
        "\n",
        "font_path = '/System/Library/Fonts/ヒラギノ角ゴシック W4.ttc' # for Mac\n",
        "# font_path = 'C:/Windows/Fonts/YuGothM.ttc' # for Windows\n",
        "try:\n",
        "    font_prop = fm.FontProperties(fname=font_path)\n",
        "    plt.rcParams['font.family'] = font_prop.get_name()\n",
        "    print(f\"日本語フォント '{font_prop.get_name()}' を設定しました。\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"警告: 指定されたフォントパス '{font_path}' が見つかりません。デフォルトフォントで描画します。\")\n",
        "\n",
        "dates = pd.date_range(start='2022-01-31', periods=37, freq='M')\n",
        "\n",
        "\n",
        "# --- Step 1: 予測データに対応する未来の日付を作成 ---\n",
        "last_historical_date = df_monthly.index[-1]\n",
        "future_dates = pd.date_range(start=last_historical_date, periods=len(future_predictions) + 1, freq='M')[1:]\n",
        "\n",
        "# --- Step 2: 予測値をPandas Seriesに変換 ---\n",
        "future_predictions_series = pd.Series(future_predictions, index=future_dates)\n",
        "\n",
        "\n",
        "# --- Step 3: グラフの描画 ---\n",
        "plt.style.use('seaborn-v0_8-whitegrid') # グラフのスタイルを指定\n",
        "plt.figure(figsize=(16, 8)) # グラフのサイズを大きく見やすく設定\n",
        "\n",
        "# 3-a: 実績値のプロット\n",
        "plt.plot(\n",
        "    df_monthly.index,\n",
        "    df_monthly['target_count_custom'],\n",
        "    label='実績値 (Historical Actuals)',\n",
        "    color='royalblue',\n",
        "    marker='o',\n",
        "    linewidth=2\n",
        ")\n",
        "\n",
        "# 3-b: 予測値のプロット\n",
        "plt.plot(\n",
        "    future_predictions_series.index,\n",
        "    future_predictions_series.values,\n",
        "    label='予測値 (Future Predictions)',\n",
        "    color='darkorange',\n",
        "    marker='x',\n",
        "    linestyle='--',\n",
        "    linewidth=2,\n",
        "    markersize=8\n",
        ")\n",
        "\n",
        "# 3-c: 実績と予測の境界線に縦線を引く\n",
        "plt.axvline(x=last_historical_date, color='gray', linestyle=':', linewidth=2, label='予測開始時点')\n",
        "\n",
        "\n",
        "# --- Step 4: グラフの装飾 ---\n",
        "plt.title('monthly_AI_prediction', fontsize=20, pad=20)\n",
        "plt.xlabel('year_date', fontsize=14)\n",
        "plt.ylabel('contract_sample', fontsize=14)\n",
        "plt.xticks(rotation=45) # 日付ラベルを斜めにして見やすくする\n",
        "plt.legend(fontsize=12) # 凡例を表示\n",
        "plt.grid(True)\n",
        "plt.tight_layout() # レイアウトを自動調整\n",
        "\n",
        "plt.savefig(\"prediction.png\", dpi=300, bbox_inches='tight')\n",
        "\n",
        "# グラフを表示\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1hk-sAgdr9HK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}